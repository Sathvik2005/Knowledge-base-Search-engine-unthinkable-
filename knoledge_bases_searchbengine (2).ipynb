{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 💫 RAG-Gemini: Next-Gen Colab Knowledge Base\n",
        "# =============================================\n",
        "!pip install -q numba==0.56.4 numpy==1.23.5\n",
        "!pip install -q langchain langchain-community faiss-cpu sentence-transformers transformers torch pypdf gradio openai-whisper\n",
        "\n",
        "import os, tempfile, io, json, traceback, shutil\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "import whisper\n",
        "\n",
        "# ---------------------\n",
        "# LangChain imports\n",
        "# ---------------------\n",
        "try:\n",
        "    from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "except:\n",
        "    from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain.vectorstores import FAISS\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# ---------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------\n",
        "CONFIG = {\n",
        "    \"chunk_size\": 400,\n",
        "    \"chunk_overlap\": 80,\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"llm_model\": \"gpt2\",\n",
        "    \"retrieval_k\": 4,\n",
        "    \"max_tokens\": 200,\n",
        "    \"device\": 0 if torch.cuda.is_available() else -1,\n",
        "    \"cache_dir\": \"./rag_gemini_cache\",\n",
        "    \"history_file\": \"./rag_gemini_history.json\",\n",
        "}\n",
        "\n",
        "os.makedirs(CONFIG[\"cache_dir\"], exist_ok=True)\n",
        "\n",
        "# ---------------------\n",
        "# UTILS\n",
        "# ---------------------\n",
        "def save_uploaded_file(uploaded_file):\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    file_path = os.path.join(temp_dir, uploaded_file.name)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file.read())\n",
        "    return file_path\n",
        "\n",
        "def load_history():\n",
        "    if os.path.exists(CONFIG[\"history_file\"]):\n",
        "        with open(CONFIG[\"history_file\"], \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return []\n",
        "\n",
        "def save_history(hist):\n",
        "    with open(CONFIG[\"history_file\"], \"w\") as f:\n",
        "        json.dump(hist, f, indent=2)\n",
        "\n",
        "# ---------------------\n",
        "# KNOWLEDGE BASE\n",
        "# ---------------------\n",
        "class KnowledgeBase:\n",
        "    def __init__(self):\n",
        "        self.docs = []\n",
        "        self.chunks = []\n",
        "        self.vectorstore = None\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])\n",
        "\n",
        "    def load_docs(self, uploaded_files):\n",
        "        self.docs = []\n",
        "        for file in uploaded_files:\n",
        "            path = save_uploaded_file(file)\n",
        "            if path.endswith(\".pdf\"):\n",
        "                docs = PyPDFLoader(path).load()\n",
        "            elif path.endswith(\".txt\"):\n",
        "                docs = TextLoader(path, encoding=\"utf-8\").load()\n",
        "            else:\n",
        "                continue\n",
        "            self.docs.extend(docs)\n",
        "        return len(self.docs)\n",
        "\n",
        "    def embed(self):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=CONFIG[\"chunk_size\"], chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
        "        )\n",
        "        self.chunks = splitter.split_documents(self.docs)\n",
        "        self.vectorstore = FAISS.from_documents(self.chunks, self.embeddings)\n",
        "        return len(self.chunks)\n",
        "\n",
        "    def retriever(self):\n",
        "        return self.vectorstore.as_retriever(search_kwargs={\"k\": CONFIG[\"retrieval_k\"]})\n",
        "\n",
        "# ---------------------\n",
        "# SYNTHESIS ENGINE\n",
        "# ---------------------\n",
        "class SynthesisEngine:\n",
        "    def __init__(self):\n",
        "        self.pipe = None\n",
        "        self.voice_model = None\n",
        "\n",
        "    def init(self):\n",
        "        self.pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=CONFIG[\"llm_model\"],\n",
        "            device=CONFIG[\"device\"],\n",
        "            max_new_tokens=CONFIG[\"max_tokens\"]\n",
        "        )\n",
        "        return True\n",
        "\n",
        "    def generate(self, query, docs, concise=False):\n",
        "        if not docs:\n",
        "            return \"No relevant context found.\"\n",
        "\n",
        "        context = \"\\n\".join([d.page_content[:600] for d in docs])\n",
        "        style = \"Give a brief, focused answer.\" if concise else \"Give a detailed, reasoned explanation.\"\n",
        "        prompt = f\"Use these DOCUMENTS to answer. If not found, say so.\\n{style}\\n\\nDOCUMENTS:\\n{context}\\n\\nQUESTION: {query}\\n\\nANSWER:\"\n",
        "        out = self.pipe(prompt, max_new_tokens=CONFIG[\"max_tokens\"], do_sample=False)\n",
        "        text = out[0][\"generated_text\"]\n",
        "        return text.split(\"ANSWER:\")[-1].strip()\n",
        "\n",
        "    def transcribe_voice(self, audio_path):\n",
        "        if self.voice_model is None:\n",
        "            self.voice_model = whisper.load_model(\"base\")\n",
        "        result = self.voice_model.transcribe(audio_path)\n",
        "        return result[\"text\"]\n",
        "\n",
        "# ---------------------\n",
        "# RAG SYSTEM\n",
        "# ---------------------\n",
        "class RAGGemini:\n",
        "    def __init__(self):\n",
        "        self.kb = KnowledgeBase()\n",
        "        self.llm = SynthesisEngine()\n",
        "        self.retriever = None\n",
        "        self.history = load_history()\n",
        "\n",
        "    def setup(self, files):\n",
        "        n = self.kb.load_docs(files)\n",
        "        c = self.kb.embed()\n",
        "        self.llm.init()\n",
        "        self.retriever = self.kb.retriever()\n",
        "        return f\"✅ {n} docs loaded & {c} chunks embedded.\"\n",
        "\n",
        "    def query(self, q, concise=False):\n",
        "        docs = self.retriever.get_relevant_documents(q)\n",
        "        answer = self.llm.generate(q, docs, concise)\n",
        "        sources = \"\\n\\n\".join([f\"📘 {d.page_content[:200]}...\" for d in docs[:3]])\n",
        "        record = {\"time\": datetime.now().isoformat(), \"query\": q, \"answer\": answer}\n",
        "        self.history.append(record)\n",
        "        save_history(self.history)\n",
        "        return answer, sources\n",
        "\n",
        "rag = RAGGemini()\n",
        "\n",
        "# ---------------------\n",
        "# UI CALLBACKS\n",
        "# ---------------------\n",
        "def build_system(files):\n",
        "    return rag.setup(files)\n",
        "\n",
        "def ask(q, concise):\n",
        "    return rag.query(q, concise)\n",
        "\n",
        "def show_history():\n",
        "    hist = rag.history[-10:][::-1]\n",
        "    return \"\\n\\n\".join([f\"🕒 {h['time']}\\n**Q:** {h['query']}\\n**A:** {h['answer']}\" for h in hist])\n",
        "\n",
        "def voice_to_text(audio):\n",
        "    temp = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "    audio.export(temp.name, format=\"wav\")\n",
        "    text = rag.llm.transcribe_voice(temp.name)\n",
        "    return text\n",
        "\n",
        "# ---------------------\n",
        "# GRADIO UI\n",
        "# ---------------------\n",
        "with gr.Blocks(css=\"\"\"\n",
        "body {background: #0e0e11; color: #e3e3e3; font-family: 'Inter', sans-serif;}\n",
        ".gr-button {background: linear-gradient(90deg, #1f2937, #374151); color: white; border-radius: 8px;}\n",
        ".gr-textbox, .gr-markdown {background-color: #111827; border-radius: 8px;}\n",
        "\"\"\") as demo:\n",
        "    gr.Markdown(\"## 🌌 **RAG-Gemini** — Intelligent Knowledge Base with Local Models\")\n",
        "    gr.Markdown(\"> Upload. Ask. Discover insights. (Runs 100% offline)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_uploader = gr.File(file_count=\"multiple\", file_types=[\".pdf\", \".txt\"], label=\"📂 Upload PDFs or Texts\")\n",
        "        build_btn = gr.Button(\"🚀 Build Knowledge Base\")\n",
        "    status_box = gr.Textbox(label=\"System Status\")\n",
        "\n",
        "    build_btn.click(build_system, inputs=[file_uploader], outputs=[status_box])\n",
        "\n",
        "    with gr.Tab(\"💬 Ask\"):\n",
        "        query_box = gr.Textbox(label=\"Ask a Question\")\n",
        "        concise = gr.Checkbox(label=\"Concise Mode\", value=False)\n",
        "        ask_btn = gr.Button(\"🔍 Query\")\n",
        "        answer_box = gr.Textbox(label=\"Answer\", lines=5)\n",
        "        sources_box = gr.Textbox(label=\"Cited Sources\", lines=5)\n",
        "        ask_btn.click(ask, inputs=[query_box, concise], outputs=[answer_box, sources_box])\n",
        "\n",
        "    with gr.Tab(\"🎙️ Voice Query\"):\n",
        "        audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\")\n",
        "        voice_btn = gr.Button(\"🎧 Transcribe\")\n",
        "        voice_text = gr.Textbox(label=\"Recognized Text\")\n",
        "        voice_btn.click(rag.llm.transcribe_voice, inputs=[audio_input], outputs=[voice_text])\n",
        "\n",
        "    with gr.Tab(\"🕘 History\"):\n",
        "        hist_btn = gr.Button(\"📜 Show Recent\")\n",
        "        hist_box = gr.Markdown()\n",
        "        hist_btn.click(show_history, outputs=[hist_box])\n",
        "\n",
        "    with gr.Tab(\"⚙️ System Info\"):\n",
        "        gr.Markdown(f\"**Embedding model:** `{CONFIG['embedding_model']}`\")\n",
        "        gr.Markdown(f\"**LLM model:** `{CONFIG['llm_model']}`\")\n",
        "        gr.Markdown(f\"**Device:** `{CONFIG['device']}`\")\n",
        "        gr.Markdown(\"Cache and embeddings are stored locally.\")\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b706106f-2839-484b-8b56-00c56120b02f",
        "id": "1_fXrPESK_4Q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hwdha5QcumP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# 💫 RAG-Gemini: Next-Gen Colab Knowledge Base\n",
        "# =============================================\n",
        "!pip install -q langchain langchain-community faiss-cpu sentence-transformers transformers torch pypdf gradio openai-whisper\n",
        "\n",
        "import os, tempfile, io, json, traceback, shutil\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "import torch\n",
        "import gradio as gr\n",
        "import whisper\n",
        "\n",
        "# ---------------------\n",
        "# LangChain imports\n",
        "# ---------------------\n",
        "try:\n",
        "    from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "except:\n",
        "    from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    from langchain.vectorstores import FAISS\n",
        "    from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# ---------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------\n",
        "CONFIG = {\n",
        "    \"chunk_size\": 400,\n",
        "    \"chunk_overlap\": 80,\n",
        "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    \"llm_model\": \"gpt2\",\n",
        "    \"retrieval_k\": 4,\n",
        "    \"max_tokens\": 200,\n",
        "    \"device\": 0 if torch.cuda.is_available() else -1,\n",
        "    \"cache_dir\": \"./rag_gemini_cache\",\n",
        "    \"history_file\": \"./rag_gemini_history.json\",\n",
        "}\n",
        "\n",
        "os.makedirs(CONFIG[\"cache_dir\"], exist_ok=True)\n",
        "\n",
        "# ---------------------\n",
        "# UTILS\n",
        "# ---------------------\n",
        "def save_uploaded_file(uploaded_file):\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    file_path = os.path.join(temp_dir, uploaded_file.name)\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(uploaded_file.read())\n",
        "    return file_path\n",
        "\n",
        "def load_history():\n",
        "    if os.path.exists(CONFIG[\"history_file\"]):\n",
        "        with open(CONFIG[\"history_file\"], \"r\") as f:\n",
        "            return json.load(f)\n",
        "    return []\n",
        "\n",
        "def save_history(hist):\n",
        "    with open(CONFIG[\"history_file\"], \"w\") as f:\n",
        "        json.dump(hist, f, indent=2)\n",
        "\n",
        "# ---------------------\n",
        "# KNOWLEDGE BASE\n",
        "# ---------------------\n",
        "class KnowledgeBase:\n",
        "    def __init__(self):\n",
        "        self.docs = []\n",
        "        self.chunks = []\n",
        "        self.vectorstore = None\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=CONFIG[\"embedding_model\"])\n",
        "\n",
        "    def load_docs(self, uploaded_files):\n",
        "        self.docs = []\n",
        "        for file in uploaded_files:\n",
        "            path = save_uploaded_file(file)\n",
        "            if path.endswith(\".pdf\"):\n",
        "                docs = PyPDFLoader(path).load()\n",
        "            elif path.endswith(\".txt\"):\n",
        "                docs = TextLoader(path, encoding=\"utf-8\").load()\n",
        "            else:\n",
        "                continue\n",
        "            self.docs.extend(docs)\n",
        "        return len(self.docs)\n",
        "\n",
        "    def embed(self):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=CONFIG[\"chunk_size\"], chunk_overlap=CONFIG[\"chunk_overlap\"]\n",
        "        )\n",
        "        self.chunks = splitter.split_documents(self.docs)\n",
        "        self.vectorstore = FAISS.from_documents(self.chunks, self.embeddings)\n",
        "        return len(self.chunks)\n",
        "\n",
        "    def retriever(self):\n",
        "        return self.vectorstore.as_retriever(search_kwargs={\"k\": CONFIG[\"retrieval_k\"]})\n",
        "\n",
        "# ---------------------\n",
        "# SYNTHESIS ENGINE\n",
        "# ---------------------\n",
        "class SynthesisEngine:\n",
        "    def __init__(self):\n",
        "        self.pipe = None\n",
        "        self.voice_model = None\n",
        "\n",
        "    def init(self):\n",
        "        self.pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=CONFIG[\"llm_model\"],\n",
        "            device=CONFIG[\"device\"],\n",
        "            max_new_tokens=CONFIG[\"max_tokens\"]\n",
        "        )\n",
        "        return True\n",
        "\n",
        "    def generate(self, query, docs, concise=False):\n",
        "        if not docs:\n",
        "            return \"No relevant context found.\"\n",
        "\n",
        "        context = \"\\n\".join([d.page_content[:600] for d in docs])\n",
        "        style = \"Give a brief, focused answer.\" if concise else \"Give a detailed, reasoned explanation.\"\n",
        "        prompt = f\"Use these DOCUMENTS to answer. If not found, say so.\\n{style}\\n\\nDOCUMENTS:\\n{context}\\n\\nQUESTION: {query}\\n\\nANSWER:\"\n",
        "        out = self.pipe(prompt, max_new_tokens=CONFIG[\"max_tokens\"], do_sample=False)\n",
        "        text = out[0][\"generated_text\"]\n",
        "        return text.split(\"ANSWER:\")[-1].strip()\n",
        "\n",
        "    def transcribe_voice(self, audio_path):\n",
        "        if self.voice_model is None:\n",
        "            self.voice_model = whisper.load_model(\"base\")\n",
        "        result = self.voice_model.transcribe(audio_path)\n",
        "        return result[\"text\"]\n",
        "\n",
        "# ---------------------\n",
        "# RAG SYSTEM\n",
        "# ---------------------\n",
        "class RAGGemini:\n",
        "    def __init__(self):\n",
        "        self.kb = KnowledgeBase()\n",
        "        self.llm = SynthesisEngine()\n",
        "        self.retriever = None\n",
        "        self.history = load_history()\n",
        "\n",
        "    def setup(self, files):\n",
        "        n = self.kb.load_docs(files)\n",
        "        c = self.kb.embed()\n",
        "        self.llm.init()\n",
        "        self.retriever = self.kb.retriever()\n",
        "        return f\"✅ {n} docs loaded & {c} chunks embedded.\"\n",
        "\n",
        "    def query(self, q, concise=False):\n",
        "        docs = self.retriever.get_relevant_documents(q)\n",
        "        answer = self.llm.generate(q, docs, concise)\n",
        "        sources = \"\\n\\n\".join([f\"📘 {d.page_content[:200]}...\" for d in docs[:3]])\n",
        "        record = {\"time\": datetime.now().isoformat(), \"query\": q, \"answer\": answer}\n",
        "        self.history.append(record)\n",
        "        save_history(self.history)\n",
        "        return answer, sources\n",
        "\n",
        "rag = RAGGemini()\n",
        "\n",
        "# ---------------------\n",
        "# UI CALLBACKS\n",
        "# ---------------------\n",
        "def build_system(files):\n",
        "    return rag.setup(files)\n",
        "\n",
        "def ask(q, concise):\n",
        "    return rag.query(q, concise)\n",
        "\n",
        "def show_history():\n",
        "    hist = rag.history[-10:][::-1]\n",
        "    return \"\\n\\n\".join([f\"🕒 {h['time']}\\n**Q:** {h['query']}\\n**A:** {h['answer']}\" for h in hist])\n",
        "\n",
        "def voice_to_text(audio):\n",
        "    temp = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
        "    audio.export(temp.name, format=\"wav\")\n",
        "    text = rag.llm.transcribe_voice(temp.name)\n",
        "    return text\n",
        "\n",
        "# ---------------------\n",
        "# GRADIO UI\n",
        "# ---------------------\n",
        "with gr.Blocks(css=\"\"\"\n",
        "body {background: #0e0e11; color: #e3e3e3; font-family: 'Inter', sans-serif;}\n",
        ".gr-button {background: linear-gradient(90deg, #1f2937, #374151); color: white; border-radius: 8px;}\n",
        ".gr-textbox, .gr-markdown {background-color: #111827; border-radius: 8px;}\n",
        "\"\"\") as demo:\n",
        "    gr.Markdown(\"## 🌌 **RAG-Gemini** — Intelligent Knowledge Base with Local Models\")\n",
        "    gr.Markdown(\"> Upload. Ask. Discover insights. (Runs 100% offline)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        file_uploader = gr.File(file_count=\"multiple\", file_types=[\".pdf\", \".txt\"], label=\"📂 Upload PDFs or Texts\")\n",
        "        build_btn = gr.Button(\"🚀 Build Knowledge Base\")\n",
        "    status_box = gr.Textbox(label=\"System Status\")\n",
        "\n",
        "    build_btn.click(build_system, inputs=[file_uploader], outputs=[status_box])\n",
        "\n",
        "    with gr.Tab(\"💬 Ask\"):\n",
        "        query_box = gr.Textbox(label=\"Ask a Question\")\n",
        "        concise = gr.Checkbox(label=\"Concise Mode\", value=False)\n",
        "        ask_btn = gr.Button(\"🔍 Query\")\n",
        "        answer_box = gr.Textbox(label=\"Answer\", lines=5)\n",
        "        sources_box = gr.Textbox(label=\"Cited Sources\", lines=5)\n",
        "        ask_btn.click(ask, inputs=[query_box, concise], outputs=[answer_box, sources_box])\n",
        "\n",
        "    with gr.Tab(\"🎙️ Voice Query\"):\n",
        "        audio_input = gr.Audio(sources=[\"microphone\"], type=\"filepath\")\n",
        "        voice_btn = gr.Button(\"🎧 Transcribe\")\n",
        "        voice_text = gr.Textbox(label=\"Recognized Text\")\n",
        "        voice_btn.click(rag.llm.transcribe_voice, inputs=[audio_input], outputs=[voice_text])\n",
        "\n",
        "    with gr.Tab(\"🕘 History\"):\n",
        "        hist_btn = gr.Button(\"📜 Show Recent\")\n",
        "        hist_box = gr.Markdown()\n",
        "        hist_btn.click(show_history, outputs=[hist_box])\n",
        "\n",
        "    with gr.Tab(\"⚙️ System Info\"):\n",
        "        gr.Markdown(f\"**Embedding model:** `{CONFIG['embedding_model']}`\")\n",
        "        gr.Markdown(f\"**LLM model:** `{CONFIG['llm_model']}`\")\n",
        "        gr.Markdown(f\"**Device:** `{CONFIG['device']}`\")\n",
        "        gr.Markdown(\"Cache and embeddings are stored locally.\")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ALr5IETdul_8",
        "outputId": "687a7b90-129d-42ed-f310-276a4a693ebf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m977.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mColab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ce80bdb7817a0bbbd5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ce80bdb7817a0bbbd5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tm1pFBNvJC6j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}